{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f44bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:35.429633Z",
     "start_time": "2022-04-26T10:58:35.422604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lesson 4 Tabular fastai - dev_nbs/course\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc1956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:35.448970Z",
     "start_time": "2022-04-26T10:58:35.438436Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *  \n",
    "from nbdev.showdoc import *   \n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4cbb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:35.468584Z",
     "start_time": "2022-04-26T10:58:35.455526Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"faroNoZero.csv\")\n",
    "# df = df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bae7ac",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Callback class change "
   ]
  },
  {
   "cell_type": "raw",
   "id": "74e26df4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "ls = []\n",
    "class CSVLogger(Callback):\n",
    "    \"Log the results displayed in `learn.path/fname`\"\n",
    "    order=60\n",
    "    def __init__(self):\n",
    "        self.fname_ = 0\n",
    "        self.NotAdded = True\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Prepare file with metric names.\"\n",
    "        if hasattr(self, \"gather_preds\"): return\n",
    "        if self.NotAdded:\n",
    "            rmse = self.learn.recorder.log[3]\n",
    "            if 0<self.fname_<100:\n",
    "                if rmse>self.fname_:\n",
    "                    # print(\"this\",f'{self.fname_}')\n",
    "                    ls.append([self.fname_,self.learn.recorder.epoch-1])\n",
    "                    self.NotAdded = False\n",
    "        else: \n",
    "            rmse=False\n",
    "        self.fname_ = rmse "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf1e13",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## From dataframe to fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623cbd11",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Understanding the classes for procs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55bccd4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T20:30:34.533741Z",
     "start_time": "2022-04-14T20:30:34.523695Z"
    },
    "hidden": true
   },
   "source": [
    "class CollBase:\n",
    "    \"Base class for composing a list of `items`\"\n",
    "    def __init__(self, items): self.items = items\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, k): return self.items[list(k) if isinstance(k,CollBase) else k]\n",
    "    def __setitem__(self, k, v): self.items[list(k) if isinstance(k,CollBase) else k] = v\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self): return self.items.__repr__()\n",
    "    def __iter__(self): return self.items.__iter__() \n",
    "    \n",
    "collbase = CollBase(df)\n",
    "collbase  \n",
    "\n",
    "'To know all the classes a class inherits from.''\n",
    "def print_root_left(class_):\n",
    "    while True:\n",
    "      print(class_)\n",
    "      # Check there if are no bases then we have reached the root class\n",
    "      if not class_.__bases__:\n",
    "        break\n",
    "      class_=class_.__bases__[0] # use the left most parent\n",
    "\n",
    "\n",
    "print_root_left(CollBase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "194520c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T12:37:21.045962Z",
     "start_time": "2022-04-21T12:37:21.026415Z"
    },
    "hidden": true
   },
   "source": [
    "#export\n",
    "class Tabular(CollBase, GetAttr, FilteredBase):\n",
    "    \"A `DataFrame` wrapper that knows which cols are cont/cat/y, and returns rows in `__getitem__`\"\n",
    "    _default,with_cont='procs',True\n",
    "    def __init__(self, df, procs=None, cat_names=None, cont_names=None, y_names=None, y_block=None, splits=None,\n",
    "                 do_setup=True, device=None, inplace=False, reduce_memory=True):\n",
    "        if inplace and splits is not None and pd.options.mode.chained_assignment is not None:\n",
    "            warn(\"Using inplace with splits will trigger a pandas error. Set `pd.options.mode.chained_assignment=None` to avoid it.\")\n",
    "        if not inplace: df = df.copy()\n",
    "        if reduce_memory: df = df_shrink(df)\n",
    "        if splits is not None: df = df.iloc[sum(splits, [])]\n",
    "        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n",
    "        super().__init__(df)\n",
    "\n",
    "        self.y_names,self.device = L(y_names),device\n",
    "        if y_block is None and self.y_names:\n",
    "            # Make ys categorical if they're not numeric\n",
    "            ys = df[self.y_names]\n",
    "            if len(ys.select_dtypes(include='number').columns)!=len(ys.columns): y_block = CategoryBlock()\n",
    "            else: y_block = RegressionBlock()\n",
    "        if y_block is not None and do_setup:\n",
    "            if callable(y_block): y_block = y_block()\n",
    "            procs = L(procs) + y_block.type_tfms\n",
    "        self.cat_names,self.cont_names,self.procs = L(cat_names),L(cont_names),Pipeline(procs)\n",
    "        self.split = len(df) if splits is None else len(splits[0])\n",
    "        if do_setup: self.setup()\n",
    "\n",
    "    def new(self, df, inplace=False):\n",
    "        return type(self)(df, do_setup=False, reduce_memory=False, y_block=TransformBlock(), inplace=inplace,\n",
    "                          **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device'))\n",
    "\n",
    "    def subset(self, i): return self.new(self.items[slice(0,self.split) if i==0 else slice(self.split,len(self))])\n",
    "    def copy(self): self.items = self.items.copy(); return self\n",
    "    def decode(self): return self.procs.decode(self)\n",
    "    def decode_row(self, row): return self.new(pd.DataFrame(row).T).decode().items.iloc[0]\n",
    "    def show(self, max_n=10, **kwargs): display_df(self.new(self.all_cols[:max_n]).decode().items)\n",
    "    def setup(self): self.procs.setup(self)\n",
    "    def process(self): self.procs(self)\n",
    "    def loc(self): return self.items.loc\n",
    "    def iloc(self): return _TabIloc(self)\n",
    "    def targ(self): return self.items[self.y_names]\n",
    "    def x_names (self): return self.cat_names + self.cont_names\n",
    "    def n_subsets(self): return 2\n",
    "    def y(self): return self[self.y_names[0]]\n",
    "    def new_empty(self): return self.new(pd.DataFrame({}, columns=self.items.columns))\n",
    "    def to_device(self, d=None):\n",
    "        self.device = d\n",
    "        return self\n",
    "\n",
    "    def all_col_names (self):\n",
    "        ys = [n for n in self.y_names if n in self.items.columns]\n",
    "        return self.x_names + self.y_names if len(ys) == len(self.y_names) else self.x_names\n",
    "\n",
    "properties(Tabular,'loc','iloc','targ','all_col_names','n_subsets','x_names','y')\n",
    "\n",
    "''' CollBase - It takes in the object of anather class (like dataframe) and applies all the dunder methods\n",
    "               to them like self.items = df ==> self.items.__repr__()\n",
    "    Tabular - Inherits from CollBase and has same dunder methods like CollBase but with some extra methods\n",
    "    TabularPandas - Inherits from Tabuar, adds one extra method i.e transform. '''\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eead0a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T20:30:35.552680Z",
     "start_time": "2022-04-14T20:30:35.547618Z"
    },
    "hidden": true
   },
   "source": [
    "#export\n",
    "class TabularPandas(Tabular):\n",
    "    \"A `Tabular` object with transforms\"\n",
    "    def transform(self, cols, f, all_col=True):\n",
    "        if not all_col: cols = [c for c in cols if c in self.items.columns]\n",
    "        if len(cols) > 0: self[cols] = self[cols].transform(f) 'this is transform of pandas'\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "99aa67e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T12:20:28.992612Z",
     "start_time": "2022-04-15T12:20:28.983904Z"
    },
    "hidden": true
   },
   "source": [
    "#export\n",
    "class TabularProc(InplaceTransform):\n",
    "    \"Base class to write a non-lazy tabular processor for dataframes\"\n",
    "    \"Transform class in fastcore also has a setup method and InplaceTransform inherits from Transform.\"\n",
    "    \"Transform class has call method which calls encode i.e calling an instace will call encode()\"\n",
    "    def setup(self, items=None, train_setup=False): #TODO: properly deal with train_setup\n",
    "        super().setup(getattr(items,'train',items), train_setup=False) 'Here to is passed in this method.'\n",
    "        'what calls this setup method? - Tabular setup func calls proc setup, as proc is passed to Tabular'\n",
    "        # Procs are called as soon as data is available  \n",
    "        return self(items.items if isinstance(items,Datasets) else items)\n",
    "\n",
    "    @property\n",
    "    def name(self): return f\"{super().name} -- {getattr(self,'__stored_args__',{})}\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "289a0b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T20:30:36.670642Z",
     "start_time": "2022-04-14T20:30:36.665710Z"
    },
    "hidden": true
   },
   "source": [
    "#export\n",
    "def _apply_cats (voc, add, c):\n",
    "    if not is_categorical_dtype(c):\n",
    "        return pd.Categorical(c, categories=voc[c.name][add:]).codes+add\n",
    "    return c.cat.codes+add #if is_categorical_dtype(c) else c.map(voc[c.name].o2i)\n",
    "def _decode_cats(voc, c): return c.map(dict(enumerate(voc[c.name].items))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84799dce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T20:30:39.314708Z",
     "start_time": "2022-04-14T20:30:39.309036Z"
    },
    "hidden": true
   },
   "source": [
    "#export\n",
    "class Categorify(TabularProc):\n",
    "    \"Transform the categorical variables to something similar to `pd.Categorical`\"\n",
    "    order = 1\n",
    "    def setups(self, to):\n",
    "        store_attr(classes={n:CategoryMap(to.iloc[:,n].items, add_na=(n in to.cat_names)) for n in to.cat_names}, but='to')\n",
    "\n",
    "    def encodes(self, to): to.transform(to.cat_names, partial(_apply_cats, self.classes, 1))\n",
    "    def decodes(self, to): to.transform(to.cat_names, partial(_decode_cats, self.classes))\n",
    "    def __getitem__(self,k): return self.classes[k]  \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "70991682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T08:53:38.437838Z",
     "start_time": "2022-04-15T08:53:38.425843Z"
    },
    "hidden": true
   },
   "source": [
    "#exporti\n",
    "@Categorize\n",
    "def setups(self, to:Tabular):  \n",
    "    '''I think this method is just for saving the variables which the \n",
    "       other 2 methods (encodes and decodes) can access to use.'''\n",
    "    if len(to.y_names) > 0:\n",
    "        if self.vocab is None:\n",
    "            self.vocab = CategoryMap(getattr(to, 'train', to).iloc[:,to.y_names[0]].items, strict=True)\n",
    "        else:\n",
    "            self.vocab = CategoryMap(self.vocab, sort=False, add_na=self.add_na)\n",
    "        self.c = len(self.vocab)\n",
    "    return self(to)\n",
    "\n",
    "@Categorize\n",
    "def encodes(self, to:Tabular):\n",
    "    to.transform(to.y_names, partial(_apply_cats, {n: self.vocab for n in to.y_names}, 0), all_col=False)\n",
    "    return to\n",
    "\n",
    "@Categorize\n",
    "def decodes(self, to:Tabular):\n",
    "    to.transform(to.y_names, partial(_decode_cats, {n: self.vocab for n in to.y_names}), all_col=False)\n",
    "    return to\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c74257",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Nomalizing the input table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1930948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T12:42:04.865038Z",
     "start_time": "2022-04-21T12:42:04.857312Z"
    },
    "hidden": true
   },
   "source": [
    "dep_var = 'CO2water'\n",
    "cat_names = [ ] # 'Month','Year'\n",
    "cont_names = ['DateJuly', 'Pt', 'Nt','Phosphate','Silicate','NO2','Amonia','CO2air','O2air','O2water','temp']\n",
    "# norm = Normalize()\n",
    "procs = [Normalize]  # maybe not normalize - Normalize\n",
    "\n",
    "val_index = int(0.8*(len(df))) \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "622baa7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T12:42:04.885335Z",
     "start_time": "2022-04-21T12:42:04.872874Z"
    },
    "hidden": true
   },
   "source": [
    "'Code for dtaframe of tuple'\n",
    "#exporti\n",
    "@Normalize\n",
    "def setups(self, to:Tabular):\n",
    "    store_attr(but='to')\n",
    "    store_attr(minimum=to.conts.min(axis=0),maximum = to.conts.max(axis=0) )\n",
    "    \n",
    "    store_attr(minimum_row=to.conts.min(axis=1),maximum_row = to.conts.max(axis=1) )\n",
    "    return self(to)  \n",
    "    '''this is a call to the instance.The changes made to self in Transform class \n",
    "       will reflect in the tabular class '''\n",
    "\n",
    "@Normalize\n",
    "def encodes(self, to:Tabular):\n",
    "    df1= (to.conts-self.minimum)/self.maximum \n",
    "    ''' This change to to.counts is returned By TabularPandas object/instance'''\n",
    "    \n",
    "    df2 = to.conts.copy()\n",
    "    df2 = df2.subtract(self.minimum_row,axis=0)\n",
    "    df2 = df2.divide(self.maximum_row,axis=0)\n",
    "    \n",
    "    for i,c in df1.iterrows():\n",
    "        for j,n in enumerate(c):\n",
    "#             print(i,j)\n",
    "            q,r = df1.iloc[i,j],df2.iloc[i,j]\n",
    "            df2.iloc[i,j] = f'{q},{r}'\n",
    "    \n",
    "#     to.conts = df2\n",
    "#     to.conts = df1.join(df2,lsuffix='_left', rsuffix='_right')\n",
    "#     to.conts = pd.concat([df1,df2])\n",
    "    'There seems to be some error with concatnation and join - due to the setter in _add_prop'\n",
    "    to.conts = df2 \n",
    "    return to\n",
    "\n",
    "@Normalize\n",
    "def decodes(self, to:Tabular):\n",
    "    to.conts = (to.conts*self.maximum ) + self.minimum\n",
    "    return to\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76554944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T18:10:43.786072Z",
     "start_time": "2022-04-17T18:10:43.760949Z"
    },
    "hidden": true
   },
   "source": [
    "'Experiment - joining two same dtaframes'\n",
    "\n",
    "df1 = pd.DataFrame(\n",
    "     {\n",
    "         \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "         \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "         \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "         \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "     },index=[0, 1, 2, 3],\n",
    " )\n",
    "    \n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "     {\n",
    "         \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"],\n",
    "         \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"],\n",
    "         \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"],\n",
    "         \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"],\n",
    "     },\n",
    "     index=[0, 1, 2, 3],\n",
    " )\n",
    " \n",
    "df1.columns = [f+'1' for f in list(df2.columns)]\n",
    "df1,df2\n",
    "# df1 = df2\n",
    "# pd.concat([df1, df2], axis=1, ignore_index=False)\n",
    "#df1, df2\n",
    "\n",
    "# for i,c in df1.iterrows():\n",
    "#     for j,n in enumerate(c):\n",
    "#     #             print(i,j)\n",
    "#         q,r = df1.iloc[i,j],df2.iloc[i,j]\n",
    "#         df2.iloc[i,j] = f'{q},{r}'\n",
    "\n",
    "# dict_ = {}\n",
    "    \n",
    "# for c in list(df2.columns):\n",
    "#     dict_[c]=[]\n",
    "#     dict_[c+'1']=[]\n",
    "\n",
    "# # (0,0),(0,2)...(1,0),(1,1)... iterrows\n",
    "# # (0,0),(1,0)...(0,1),(1,1)... iteritems \n",
    "\n",
    "# for i,c in df2.iteritems():\n",
    "#     for j,n in enumerate(c): \n",
    "#         n = n.split(',')\n",
    "#         dict_[i].append(n[0])\n",
    "#         dict_[i+'1'].append(n[1])\n",
    "#         print(n[0],n[1])\n",
    "\n",
    "# pd.DataFrame(dict_),df2 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "34118267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T10:15:35.137032Z",
     "start_time": "2022-04-17T10:15:35.117987Z"
    },
    "hidden": true
   },
   "source": [
    "# Cell\n",
    "class FilteredBase:\n",
    "    \"Base class for lists with subsets\"\n",
    "    _dl_type,_dbunch_type = TfmdDL,DataLoaders\n",
    "    def __init__(self, *args, dl_type=None, **kwargs):\n",
    "        if dl_type is not None: self._dl_type = dl_type\n",
    "        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def n_subsets(self): return len(self.splits)\n",
    "    def _new(self, items, **kwargs): return super()._new(items, splits=self.splits, **kwargs)\n",
    "    def subset(self): raise NotImplemented\n",
    "\n",
    "    def dataloaders(self, bs=64, shuffle_train=None, shuffle=True, val_shuffle=False,n=None, path='.', dl_type=None, dl_kwargs=None,\n",
    "                    device=None,drop_last=None,val_bs=None, **kwargs):\n",
    "        if shuffle_train is not None:\n",
    "            shuffle=shuffle_train\n",
    "            warnings.warn('`shuffle_train` is deprecated. Use `shuffle` instead.',DeprecationWarning)\n",
    "        if device is None: device=default_device()\n",
    "        if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets\n",
    "        if dl_type is None: dl_type = self._dl_type\n",
    "        if drop_last is None: drop_last = shuffle\n",
    "        val_kwargs={k[4:]:v for k,v in kwargs.items() if k.startswith('val_')}\n",
    "        def_kwargs = {'bs':bs,'shuffle':shuffle,'drop_last':drop_last,'n':n,'device':device}\n",
    "        dl = dl_type(self.subset(0), **merge(kwargs,def_kwargs, dl_kwargs[0]))\n",
    "        def_kwargs = {'bs':bs if val_bs is None else val_bs,'shuffle':val_shuffle,'n':None,'drop_last':False}\n",
    "        dls = [dl] + [dl.new(self.subset(i), **merge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n",
    "                      for i in range(1, self.n_subsets)]\n",
    "        return self._dbunch_type(*dls, path=path, device=device)\n",
    "\n",
    "FilteredBase.train,FilteredBase.valid = add_props(lambda i,x: x.subset(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2437c826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T11:44:47.891948Z",
     "start_time": "2022-04-19T11:44:47.879761Z"
    },
    "hidden": true
   },
   "source": [
    "'Code for concatenated dtaframe'\n",
    "#exporti\n",
    "@Normalize\n",
    "def setups(self, to:Tabular):\n",
    "    store_attr(but='to')\n",
    "    store_attr(minimum=to.conts.min(axis=0),maximum = to.conts.max(axis=0) )\n",
    "    \n",
    "    store_attr(minimum_row=to.conts.min(axis=1),maximum_row = to.conts.max(axis=1) )\n",
    "    return self(to)  \n",
    "    '''this is a call to the instance.The changes made to self in Transform class \n",
    "       will reflect in the tabular class '''\n",
    "\n",
    "@Normalize\n",
    "def encodes(self, to:Tabular):\n",
    "    df1= (to.conts-self.minimum)/self.maximum \n",
    "    '''This change to to.counts is returned By TabularPandas object/instance'''\n",
    "    \n",
    "    df2 = to.conts.copy()\n",
    "    df2 = df2.subtract(self.minimum_row,axis=0)\n",
    "    df2 = df2.divide(self.maximum_row,axis=0)\n",
    "    df2.columns = [f+'1' for f in list(df2.columns)]\n",
    "#     to = pd.concat([df1,df2], axis=1,ignore_index=False)\n",
    "#     setattr(to,'conts',pd.concat([df1,df2], axis=1,ignore_index=False))\n",
    "    to.conts = pd.concat([df1,df2], axis=1,ignore_index=False)\n",
    "    \n",
    "    print(to.conts,\"this is the attribute\") \n",
    "    'was making the dataframe equal to to.conts, which should have been equal to to' \n",
    "    return to\n",
    "\n",
    "@Normalize\n",
    "def decodes(self, to:Tabular):\n",
    "    to.conts = (to.conts*self.maximum ) + self.minimum\n",
    "    return to\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb9e73ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T11:44:47.911378Z",
     "start_time": "2022-04-19T11:44:47.902513Z"
    },
    "hidden": true
   },
   "source": [
    "def _add_prop(cls, nm):\n",
    "    @property\n",
    "    def f(o): return o[list(getattr(o,nm+'_names'))]\n",
    "    @f.setter\n",
    "    def fset(o, v): o[getattr(o,nm+'_names')] = v \n",
    "    'v is the df i am passing and it doesn\"t match with input df'\n",
    "    setattr(cls, nm+'s', f)\n",
    "    setattr(cls, nm+'s', fset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d0d2c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T17:59:33.476625Z",
     "start_time": "2022-04-17T17:59:33.444011Z"
    },
    "hidden": true
   },
   "source": [
    "def _maybe_expand(o): return o[:,None] if o.ndim==1 else o\n",
    "\n",
    "@ReadTabBatch  \n",
    "def encodes(self, to):\n",
    "    dict_ = {}\n",
    "\n",
    "    for c in list(to.cats.columns):\n",
    "        dict_[c]=[]\n",
    "        dict_[c+'1']=[]\n",
    "\n",
    "    # (0,0),(0,2)...(1,0),(1,1)... iterrows\n",
    "    # (0,0),(1,0)...(0,1),(1,1)... iteritems \n",
    "\n",
    "    for i,c in to.cats.iteritems():\n",
    "        for j,n in enumerate(c): \n",
    "            n = n.split(',')\n",
    "            dict_[i].append(n[0])\n",
    "            dict_[i+'1'].append(n[1])\n",
    "            print(n[0],n[1]) \n",
    "\n",
    "    df = pd.DataFrame(dict_)\n",
    "    if not to.with_cont: res = (tensor(to.cats).long(),)    \n",
    "    else: res = (tensor(to.cats).long(),tensor(df).float())\n",
    "    ys = [n for n in to.y_names if n in to.items.columns]\n",
    "    if len(ys) == len(to.y_names): res = res + (tensor(to.targ),)\n",
    "    if to.device is not None: res = to_device(res, to.device)\n",
    "    return res\n",
    "\n",
    "@ReadTabBatch  \n",
    "def decodes(self, o):\n",
    "    o = [_maybe_expand(o_) for o_ in to_np(o) if o_.size != 0]\n",
    "    vals = np.concatenate(o, axis=1)\n",
    "    print(vals,self.to.x_names)\n",
    "    try: df = pd.DataFrame(vals, columns=self.to.all_col_names)\n",
    "    except: df = pd.DataFrame(vals, columns=self.to.x_names)\n",
    "    to = self.to.new(df) \n",
    "    'can try to assign a new DF to \"to\", earlier i was trying to assing df to to.conts'\n",
    "    return to\n",
    "\n",
    "'No need to write the full class, jsut write the function which are needed and delegate them to the class.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f4138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:35.593733Z",
     "start_time": "2022-04-26T10:58:35.587524Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# @ReadTabBatch  \n",
    "# def encodes(self, to):\n",
    "#     if not to.with_cont: res = (tensor(to.cats).long(),)\n",
    "#     else: \n",
    "#         res = (tensor(to.cats).long(),tensor(to.conts).float())\n",
    "#         # print(to.conts)\n",
    "#     ys = [n for n in to.y_names if n in to.items.columns]\n",
    "#     if len(ys) == len(to.y_names): res = res + (tensor(to.targ),)\n",
    "#     if to.device is not None: res = to_device(res, to.device)\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84a4f480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T12:42:04.933493Z",
     "start_time": "2022-04-21T12:42:04.912151Z"
    },
    "hidden": true
   },
   "source": [
    "# @delegates()\n",
    "# class FilteredBase:\n",
    "#     \"Base class for lists with subsets\"\n",
    "#     _dl_type,_dbunch_type = TfmdDL,DataLoaders\n",
    "#     def __init__(self, *args, dl_type=None, **kwargs):\n",
    "#         if dl_type is not None: self._dl_type = dl_type\n",
    "#         self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n",
    "#         super().__init__(*args, **kwargs)\n",
    "\n",
    "#     @property\n",
    "#     def n_subsets(self): return len(self.splits)\n",
    "#     def _new(self, items, **kwargs): return super()._new(items, splits=self.splits, **kwargs)\n",
    "#     def subset(self): raise NotImplemented\n",
    "\n",
    "def dataloaders(self, bs=64, shuffle_train=None, shuffle=True, val_shuffle=False,n=None, path='.', dl_type=None, dl_kwargs=None,\n",
    "                device=None,drop_last=None,val_bs=None, **kwargs):\n",
    "    if shuffle_train is not None:\n",
    "        shuffle=shuffle_train\n",
    "        warnings.warn('`shuffle_train` is deprecated. Use `shuffle` instead.',DeprecationWarning)\n",
    "    if device is None: device=default_device()\n",
    "    if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets\n",
    "    if dl_type is None: dl_type = self._dl_type\n",
    "    if drop_last is None: drop_last = shuffle\n",
    "    val_kwargs={k[4:]:v for k,v in kwargs.items() if k.startswith('val_')}\n",
    "    def_kwargs = {'bs':bs,'shuffle':shuffle,'drop_last':drop_last,'n':n,'device':device}\n",
    "    dl = dl_type(self.subset(0), **merge(kwargs,def_kwargs, dl_kwargs[0]))\n",
    "    print(self.subset(0).conts)\n",
    "    def_kwargs = {'bs':bs if val_bs is None else val_bs,'shuffle':val_shuffle,'n':None,'drop_last':False}\n",
    "    dls = [dl] + [dl.new(self.subset(i), **merge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n",
    "                  for i in range(1, self.n_subsets)]\n",
    "    return self._dbunch_type(*dls, path=path, device=device)\n",
    "    \n",
    "FilteredBase.dataloaders = dataloaders  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0676aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T09:49:54.607907Z",
     "start_time": "2022-04-21T09:49:54.602569Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160c63b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T09:49:54.632749Z",
     "start_time": "2022-04-21T09:49:54.622125Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0b3bcdcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T12:42:05.754969Z",
     "start_time": "2022-04-21T12:42:04.939850Z"
    },
    "hidden": true
   },
   "source": [
    "splits = IndexSplitter(list(range(val_index+1,len(df))))(range_of(df))\n",
    "to = TabularPandas(df, procs, cat_names, cont_names, y_names=dep_var, splits=splits) \n",
    "'''The self object(df) of TbaularPandas has a proc attribute.This proc attribute is passed the self and \n",
    "    some changes are made to this self in proc class and these changes are carried in place in the Tabular \n",
    "    class. '''\n",
    "# dls = to.dataloaders(bs=16)\n",
    "# dls.show_batch() \n",
    "# dls.xs[1:3] \n",
    "# to.train.conts \n",
    "# to \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29e1af",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## From array to fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60f9d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:37.294062Z",
     "start_time": "2022-04-26T10:58:35.618641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df is without labels\n",
    "df_ = df.iloc[:,:-1]\n",
    "\n",
    "minimum=df_.min(axis=0)\n",
    "maximum = df_.max(axis=0) \n",
    "    \n",
    "minimum_row=df_.min(axis=1)\n",
    "maximum_row = df_.max(axis=1) \n",
    "\n",
    "df1= (df_-minimum)/maximum \n",
    "''' This change to to.counts is returned By TabularPandas object/instance'''\n",
    "    \n",
    "df2 = df_.copy()\n",
    "df2 = df2.subtract(minimum_row,axis=0)\n",
    "df2 = df2.divide(maximum_row,axis=0)\n",
    "\n",
    "for i,c in df1.iterrows():\n",
    "    for j,n in enumerate(c):\n",
    "#             print(i,j)\n",
    "        q,r = df1.iloc[i,j],df2.iloc[i,j]\n",
    "        df2.iloc[i,j] = f'{q},{r}'\n",
    "\n",
    "\n",
    "k_ = 16\n",
    "Combination_list = [2,3,4]\n",
    "\n",
    "combinationDF = df2\n",
    "\n",
    "array_list = []\n",
    "\n",
    "for c_ in range(combinationDF.shape[0]):\n",
    "    original_list = list(combinationDF.iloc[c_,:])\n",
    "\n",
    "    holding_dict = {'hold_combinations2': [],'hold_combinations3' : [],'hold_combinations4' : []}\n",
    "    holding_dict_ = {'hold_combinations2': [],'hold_combinations3' : [],'hold_combinations4' : []}\n",
    "\n",
    "    column_normalized = []\n",
    "    row_normalized = []\n",
    "    for x in original_list:\n",
    "        column_normalized.append(float(x.split(',')[0]))\n",
    "        row_normalized.append(float(x.split(',')[1]))\n",
    "\n",
    "\n",
    "\n",
    "    for CombinationLength in Combination_list:\n",
    "        for k in range(k_):\n",
    "\n",
    "            l = np.random.choice(column_normalized, size=CombinationLength, replace=True)\n",
    "            l_ = np.random.choice(row_normalized, size=CombinationLength, replace=True)\n",
    "\n",
    "            holding_dict['hold_combinations'+f'{CombinationLength}'].append(l)\n",
    "            holding_dict_['hold_combinations'+f'{CombinationLength}'].append(l_)\n",
    "\n",
    "    a = np.array(holding_dict['hold_combinations2']).reshape(1,-1)\n",
    "    b = np.array(holding_dict['hold_combinations3']).reshape(1,-1)\n",
    "    c = np.array(holding_dict['hold_combinations4']).reshape(1,-1)\n",
    "\n",
    "    a_ = np.array(holding_dict_['hold_combinations2']).reshape(1,-1)\n",
    "    b_ = np.array(holding_dict_['hold_combinations3']).reshape(1,-1)\n",
    "    c_ = np.array(holding_dict_['hold_combinations4']).reshape(1,-1)\n",
    "\n",
    "    A = np.concatenate([a,a_],axis=0)\n",
    "    B = np.concatenate([b,b_],axis=0)\n",
    "    C = np.concatenate([c,c_],axis=0)\n",
    "    # B = np.array([b,b_])\n",
    "    # C = np.array([c,c_])\n",
    "\n",
    "    arraylist = [A,B,C]\n",
    "    outarr = np.ones((2*len(arraylist),np.max([ps.shape[1] for ps in arraylist])))*np.nan #define empty array\n",
    "    for i,c in enumerate(arraylist):  #populate columns\n",
    "        outarr[2*(i):2*i+2,:c.shape[1]]=c\n",
    "\n",
    "    array_list.append(outarr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789aca22",
   "metadata": {},
   "source": [
    "## DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5c176",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84b75b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:37.315108Z",
     "start_time": "2022-04-26T10:58:37.298246Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *  \n",
    "from nbdev.showdoc import *   \n",
    "from fastai.vision.all import *\n",
    "\n",
    "df = pd.read_csv(\"faroNoZero.csv\")\n",
    "df = df.iloc[:,2:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b6885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:37.353663Z",
     "start_time": "2022-04-26T10:58:37.320155Z"
    }
   },
   "outputs": [],
   "source": [
    "#df is without labels\n",
    "class DfToArray(Transform):\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "    \n",
    "    def encodes(self,i_):\n",
    "        # print(i_)\n",
    "        df_ = self.df.iloc[:,:-1]\n",
    "\n",
    "        minimum=df_.min(axis=0)\n",
    "        maximum = df_.max(axis=0) \n",
    "\n",
    "        minimum_row=df_.min(axis=1)\n",
    "        maximum_row = df_.max(axis=1) \n",
    "\n",
    "        df1= (df_-minimum)/maximum \n",
    "        ''' This change to to.counts is returned By TabularPandas object/instance'''\n",
    "\n",
    "        df2 = df_.copy()\n",
    "        df2 = df2.subtract(minimum_row,axis=0)\n",
    "        df2 = df2.divide(maximum_row,axis=0)\n",
    "\n",
    "        for i,c in df1.iterrows():\n",
    "            for j,n in enumerate(c):\n",
    "        #             print(i,j)\n",
    "                q,r = df1.iloc[i,j],df2.iloc[i,j]\n",
    "                df2.iloc[i,j] = f'{q},{r}'\n",
    "\n",
    "\n",
    "        k_ = 15\n",
    "        Combination_list = [2,3,4]\n",
    "\n",
    "        combinationDF = df2\n",
    "\n",
    "        # array_list = []\n",
    "\n",
    "    #     for c_ in range(combinationDF.shape[0]):\n",
    "        original_list = list(combinationDF.iloc[i_,:])\n",
    "\n",
    "        holding_dict = {'hold_combinations2': [],'hold_combinations3' : [],'hold_combinations4' : []}\n",
    "        holding_dict_ = {'hold_combinations2': [],'hold_combinations3' : [],'hold_combinations4' : []}\n",
    "\n",
    "        column_normalized = []\n",
    "        row_normalized = []\n",
    "        for x in original_list:\n",
    "            column_normalized.append(float(x.split(',')[0]))\n",
    "            row_normalized.append(float(x.split(',')[1]))\n",
    "\n",
    "\n",
    "\n",
    "        for CombinationLength in Combination_list:\n",
    "            for k in range(k_):\n",
    "\n",
    "                l = np.random.choice(column_normalized, size=CombinationLength, replace=True)\n",
    "                l_ = np.random.choice(row_normalized, size=CombinationLength, replace=True)\n",
    "\n",
    "                holding_dict['hold_combinations'+f'{CombinationLength}'].append(l)\n",
    "                holding_dict_['hold_combinations'+f'{CombinationLength}'].append(l_)\n",
    "\n",
    "        a = np.array(holding_dict['hold_combinations2']).reshape(1,-1)\n",
    "        b = np.array(holding_dict['hold_combinations3']).reshape(1,-1)\n",
    "        c = np.array(holding_dict['hold_combinations4']).reshape(1,-1)\n",
    "\n",
    "        a_ = np.array(holding_dict_['hold_combinations2']).reshape(1,-1)\n",
    "        b_ = np.array(holding_dict_['hold_combinations3']).reshape(1,-1)\n",
    "        c_ = np.array(holding_dict_['hold_combinations4']).reshape(1,-1)\n",
    "\n",
    "        A = np.concatenate([a,a_],axis=0)\n",
    "        B = np.concatenate([b,b_],axis=0)\n",
    "        C = np.concatenate([c,c_],axis=0)\n",
    "        # B = np.array([b,b_])\n",
    "        # C = np.array([c,c_])\n",
    "\n",
    "        arraylist = [A,B,C]\n",
    "        outarr = np.ones((2*len(arraylist),np.max([ps.shape[1] for ps in arraylist])))*np.nan #define empty array\n",
    "        for i,c in enumerate(arraylist):  #populate columns\n",
    "            outarr[2*(i):2*i+2,:c.shape[1]]=c\n",
    "\n",
    "        # array_list.append(outarr)\n",
    "#         print(outarr)\n",
    "        return outarr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0250e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:38.762146Z",
     "start_time": "2022-04-26T10:58:37.361420Z"
    }
   },
   "outputs": [],
   "source": [
    "def DfToArrayBl(df_): \n",
    "    return TransformBlock(type_tfms=DfToArray(df_)) \n",
    "\n",
    "'this used first for creating the file names/indexes' \n",
    "def get_index(files):  \n",
    "    return [f for f in range(files)]\n",
    "\n",
    "def get_y(files):\n",
    "    return df.iloc[files,-1]\n",
    "\n",
    "'Automatically produced 80:20 tain:valid datasets'\n",
    "siamese = DataBlock(\n",
    "    blocks=(DfToArrayBl(df), RegressionSetup),\n",
    "    get_items=get_index,get_y=get_y)\n",
    "\n",
    "dls = siamese.dataloaders(source = len(df),bs=16,shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9a15d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:39.717058Z",
     "start_time": "2022-04-26T10:58:38.766164Z"
    }
   },
   "outputs": [],
   "source": [
    "for vb in dls.valid_ds:\n",
    "#     print(vb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a363fa0",
   "metadata": {},
   "source": [
    "## Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6b0e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:39.969510Z",
     "start_time": "2022-04-26T10:58:39.954543Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Checking the individual layers'''\n",
    "a = torch.randn(2, 30)  # batch,channnel,width\n",
    "# m = ConvLayer(ni=2, nf=256, ks=2, stride=2,ndim=1,act_cls=nn.SELU)  #\n",
    "m = LinBnDrop(30,10,bn=True,p=0.2,act=nn.ReLU(),lin_first=True)\n",
    "out = m(a)\n",
    "print(a.size(),out.size()) \n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:39.991715Z",
     "start_time": "2022-04-26T10:58:39.975586Z"
    }
   },
   "outputs": [],
   "source": [
    "for n,x in m.named_parameters():\n",
    "    print(n,x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77694142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:40.081965Z",
     "start_time": "2022-04-26T10:58:40.065906Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Creating new tensors for testing'''\n",
    "n_grams = vb[0]\n",
    "n_grams_tensor = torch.Tensor(n_grams)\n",
    "n_grams_tensor = torch.randn(2,6, 60)\n",
    "# n_grams_tensor_ = torch.randn(2,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f94af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:40.156565Z",
     "start_time": "2022-04-26T10:58:40.087787Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransferEncoder(nn.Module):\n",
    "    def __init__(self,outchannels = 256 , k = 16,nGrams = 3,secondconv = [3,3,2],ndim_=1,activation_=nn.SELU,\n",
    "                dense=6144,linear_layers=3,linFirst=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.outchannels,self.secondconv = outchannels,secondconv[0] \n",
    "        self.SecondConvKS,self.SecondConvStride = secondconv[1],secondconv[2]\n",
    "        self.k,self.nGrams,self.ndim,self.activation,self.dense = k,nGrams,ndim_,activation_,dense\n",
    "        \n",
    "        some = 0\n",
    "        for n in range(30):\n",
    "            if 2**n>self.dense:\n",
    "                some += n\n",
    "                break\n",
    "        \n",
    "        # 'list_1_ = [ nn.Conv1d(in_channels=2, out_channels=self.outchannels,kernel_size=ks, stride=ks)for ks in [i for i in range(2,2+nGrams)]  ]'\n",
    "        list_1_ = [ ConvLayer(ni=2, nf=self.outchannels, ks=ks, stride=ks,padding=0,ndim=self.ndim,act_cls=self.activation)\n",
    "                   for ks in [i for i in range(2,2+nGrams)]  ]\n",
    "        self.firstconvs = nn.ModuleList(list_1_)\n",
    "        \n",
    "        # 'list_ = [nn.Conv1d(in_channels=inchanel, out_channels=inchanel*2,kernel_size=self.SecondConvKS,stride=self.SecondConvStride) for inchanel in [self.nGrams*self.outchannels*((2)**u) for u in range(self.secondconv)] ]'\n",
    "        'Can remove self.secondconv-1 to self.secondconv as with batch everything works fine.'\n",
    "        self.list_ = [ ConvLayer(ni=inchanel, nf=inchanel*2, ks=self.SecondConvKS, stride=self.SecondConvStride,padding=0,ndim=self.ndim,act_cls=self.activation,norm_type=None) \n",
    "                      if last==(self.secondconv-1) \n",
    "                      else \n",
    "                      ConvLayer(ni=inchanel, nf=inchanel*2, ks=self.SecondConvKS, stride=self.SecondConvStride,padding=0,ndim=self.ndim,act_cls=self.activation) \n",
    "                      for last, inchanel in enumerate([self.nGrams*self.outchannels*((2)**u) for u in range(self.secondconv)]) ]\n",
    "        self.secondconvs = nn.Sequential(*self.list_)\n",
    "        \n",
    "        # self.linear1 = nn.Linear(6144)\n",
    "        denselayer = [[LinBnDrop(dense,2**(some-d),bn=True,p=0.2,act=nn.ReLU(),lin_first=linFirst),\n",
    "                            LinBnDrop(2**(some-d),2**(some-d-2),bn=True,p=0.2,act=nn.ReLU(),lin_first=linFirst)]\n",
    "                            if d==2\n",
    "                            else \n",
    "                            [LinBnDrop(2**(some-d),2**(some-d-2),bn=True,p=0.2,act=nn.ReLU(),lin_first=linFirst)]\n",
    "                            for d in range(2,int(2*linear_layers),2)]\n",
    "        self.denselayer_ = sum(denselayer,[])\n",
    "        self.EncoderDense = nn.Sequential(*self.denselayer_)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        NgramConv = torch.empty(size=(x.shape[0],3*self.outchannels,self.k))\n",
    "        \n",
    "        for i,y in zip(range(3) ,self.firstconvs ):\n",
    "            eachGram =  x[:,i*2:(i+1)*2 , :self.k*(i+2)]\n",
    "            'Not sure if this above line is needed in the training with the batch dimention provided'\n",
    "            x_ = y( eachGram ) \n",
    "            NgramConv[:,i*self.outchannels:(i+1)*self.outchannels,:]= x_\n",
    "                    \n",
    "        dense = self.secondconvs(NgramConv)\n",
    "        dense.squeeze_(2)\n",
    "        'look at this also ' \n",
    "        \n",
    "        dense = self.EncoderDense(dense)\n",
    "\n",
    "        return dense\n",
    "\n",
    "\n",
    "class TransferClassifier(nn.Module):\n",
    "    def __init__(self,ndim_=1,activation_=nn.ReLU(),dense=128,linear_layers=3,last_layer=True,linFirst=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.linear_layers_=linear_layers+1\n",
    "        some = 0\n",
    "        for n in range(30):\n",
    "            if 2**n>dense:\n",
    "                some += n\n",
    "                break\n",
    "        \n",
    "        denselayer = [[LinBnDrop(dense,2**(some-d),bn=True,p=0.2,act=activation_,lin_first=linFirst),\n",
    "                       LinBnDrop(2**(some-d),2**(some-d-1),bn=True,p=0.2,act=activation_,lin_first=linFirst)]\n",
    "                      if d==2\n",
    "                      else \n",
    "                      [LinBnDrop(2**(some-d),2**(some-d-1),bn=True,p=0.2,act=activation_,lin_first=linFirst)] \n",
    "                      for d in range(2,int(self.linear_layers_),1)]\n",
    "        self.denselayer_ = sum(denselayer,[])\n",
    "        \n",
    "        last = int(dense/2**(linear_layers))\n",
    "        if last>1 and last_layer:\n",
    "            self.denselayer_.append(LinBnDrop(last,1,bn=True,p=0.2,act=activation_,lin_first=linFirst))\n",
    "        self.ClassifierDense = nn.Sequential(*self.denselayer_)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        NgramConv = self.ClassifierDense(x)\n",
    "        return NgramConv\n",
    "\n",
    "\n",
    "# NgramConv_ = model(n_grams_tensor)\n",
    "# NgramConv_ = model_(NgramConv_)\n",
    "\n",
    "class TransferTabular(nn.Module):\n",
    "    def __init__(self,encoder,classifier):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9446b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:40.311452Z",
     "start_time": "2022-04-26T10:58:40.161411Z"
    }
   },
   "outputs": [],
   "source": [
    "encode = TransferEncoder(k = 15)        \n",
    "classify = TransferClassifier()\n",
    "model = TransferTabular(encode,classify)\n",
    "NgramConv_ = model(n_grams_tensor)\n",
    "# NgramConv_.shape,model\n",
    "# tensor_reshaped = tensor_reshaped[~torch.any(tensor_reshaped.isnan(),dim=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55199fe2",
   "metadata": {},
   "source": [
    "## Checking the transfer learnign mechanism\n",
    "It seems that below mensioned method is the best option.\n",
    "1. Saving the privious model\n",
    "2. Loading it \n",
    "3. Taking out its encoder with children list\n",
    "4. Then using that encoder with TransferClassifier() inside TransferTabular to create a new model.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8804af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:40.328555Z",
     "start_time": "2022-04-26T10:58:40.328514Z"
    }
   },
   "outputs": [],
   "source": [
    "# for n,x in model.encoder.named_parameters():\n",
    "#     print(n)\n",
    "# for n in IndividualEncoder.named_parameters():\n",
    "#     print(n)\n",
    "\n",
    "IndividualEncoder = list(model.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8d75e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:40.332291Z",
     "start_time": "2022-04-26T10:58:40.332250Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Creating new model'''\n",
    "classify1 = TransferClassifier()\n",
    "model1 = TransferTabular(IndividualEncoder,classify1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200d6dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:40.339449Z",
     "start_time": "2022-04-26T10:58:40.339390Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Taking the encoder of the new model'''\n",
    "IndividualEncoder1 = list(model1.children())[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284df83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:40.345878Z",
     "start_time": "2022-04-26T10:58:40.345836Z"
    }
   },
   "outputs": [],
   "source": [
    "'''checking that the encoder weights of the old and the new model are the same'''\n",
    "'''eval for negating dropouts'''\n",
    "# for (n,x),(n1,x1) in zip(model.classifier.named_parameters(),model1.classifier.named_parameters()):\n",
    "#     print(x==x1)\n",
    "#     break\n",
    "model.encoder.eval()(n_grams_tensor),model1.encoder.eval()(n_grams_tensor),\n",
    "# IndividualEncoder.eval()(n_grams_tensor),IndividualEncoder1.eval()(n_grams_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ba700",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Not Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a78fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T10:58:40.351340Z",
     "start_time": "2022-04-26T10:58:40.351298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fully connnected network without cross validation.\n",
    "\n",
    "# splits the toal index into train and valid\n",
    "# IndexSplitter(x,y)(z)\n",
    "# takes in list z and splits it into (0 to x) and (x to y) \n",
    "\n",
    "\n",
    "splits = IndexSplitter(list(range(val_index+1,len(df))))(range_of(df))\n",
    "to = TabularPandas(df, procs, cat_names, cont_names, y_names=dep_var, splits=splits) \n",
    "dls = to.dataloaders(bs=16) \n",
    "dls.show_batch()\n",
    "learn = tabular_learner(dls, layers=[500,300,300], metrics=rmse)  \n",
    "learn.fit(26, 1e-2,cbs=[CSVLogger()])   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.7.0",
   "language": "python",
   "name": "3.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "411.98370361328125px",
    "left": "915.9239501953125px",
    "right": "20px",
    "top": "120px",
    "width": "296.5081481933594px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
